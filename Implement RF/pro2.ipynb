{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现cache友好，进行数据随机打乱，然后重新写入硬盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for i in range(1, 6):\n",
    "    df = pd.read_csv('./data/train' + str(i) + '.csv', header=None)\n",
    "    lb = pd.read_csv('./data/label' + str(i) + '.csv', header=None)\n",
    "    df[13] = lb[0]\n",
    "    dfs.append(df)\n",
    "    \n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0      1      2      3      4      5      6         7   \\\n",
      "0       -0.221230 -218.0  -15.0  151.0   -1.0  -74.0  -65.0  0.458700   \n",
      "1       -0.505680    2.0 -218.0  -15.0  151.0   -1.0  -74.0 -0.490380   \n",
      "2       -0.073529  -10.5    2.0 -218.0  -15.0  151.0   -1.0  0.435190   \n",
      "3        1.213900  -54.0  -10.5    2.0 -218.0  -15.0  151.0  0.849830   \n",
      "4        0.482010   45.0  -54.0  -10.5    2.0 -218.0  -15.0  0.363640   \n",
      "5       -0.247710   -5.0   45.0  -54.0  -10.5    2.0 -218.0  0.958160   \n",
      "6       -0.041667 -343.0   -5.0   45.0  -54.0  -10.5    2.0 -0.266360   \n",
      "7       -0.404760 -326.0 -343.0   -5.0   45.0  -54.0  -10.5 -0.905350   \n",
      "8       -0.696840  -47.0 -326.0 -343.0   -5.0   45.0  -54.0  0.763820   \n",
      "9        0.109760   -1.0  -47.0 -326.0 -343.0   -5.0   45.0  0.825610   \n",
      "10       0.250000  -46.0   -1.0  -47.0 -326.0 -343.0   -5.0  0.644110   \n",
      "11      -0.361840 -328.0  -46.0   -1.0  -47.0 -326.0 -343.0 -0.336490   \n",
      "12       1.496500 -246.0 -328.0  -46.0   -1.0  -47.0 -326.0  0.562280   \n",
      "13       0.020468 -510.0 -246.0 -328.0  -46.0   -1.0  -47.0  0.082993   \n",
      "14       0.140630    9.0 -510.0 -246.0 -328.0  -46.0   -1.0  0.094996   \n",
      "15      -0.183540  -43.0    9.0 -510.0 -246.0 -328.0  -46.0  0.066971   \n",
      "16      -0.500000  -85.0  -43.0    9.0 -510.0 -246.0 -328.0  0.002383   \n",
      "17      -0.460490 -326.0  -85.0  -43.0    9.0 -510.0 -246.0 -0.336110   \n",
      "18      -0.477710 -303.0 -326.0  -85.0  -43.0    9.0 -510.0 -0.963080   \n",
      "19       0.000000 -114.0 -303.0 -326.0  -85.0  -43.0    9.0  0.582420   \n",
      "20       0.064444  -61.0 -114.0 -303.0 -326.0  -85.0  -43.0  0.634570   \n",
      "21      -0.340910 -165.0  -61.0 -114.0 -303.0 -326.0  -85.0  0.258430   \n",
      "22      -0.200930 -154.0 -165.0  -61.0 -114.0 -303.0 -326.0 -0.145540   \n",
      "23      -0.313950  -58.0 -154.0 -165.0  -61.0 -114.0 -303.0 -0.289860   \n",
      "24      -0.406540  -39.0  -58.0 -154.0 -165.0  -61.0 -114.0 -0.430890   \n",
      "25       0.046296 -105.0  -39.0  -58.0 -154.0 -165.0  -61.0  0.889470   \n",
      "26       0.986380   92.0 -105.0  -39.0  -58.0 -154.0 -165.0  0.985620   \n",
      "27       0.006897    6.0   92.0 -105.0  -39.0  -58.0 -154.0 -0.901640   \n",
      "28      -0.060000  -10.0    6.0   92.0 -105.0  -39.0  -58.0  0.988950   \n",
      "29       0.032258  112.0  -10.0    6.0   92.0 -105.0  -39.0 -0.062762   \n",
      "...           ...    ...    ...    ...    ...    ...    ...       ...   \n",
      "1999971 -0.500000  -14.0  -24.0 -311.0    0.0  -10.0   -7.0 -0.770750   \n",
      "1999972 -0.394740  -17.0  -14.0  -24.0 -311.0    0.0  -10.0 -0.811320   \n",
      "1999973 -0.409090  -12.0  -17.0  -14.0  -24.0 -311.0    0.0 -0.845730   \n",
      "1999974  0.000000 -247.0  -12.0  -17.0  -14.0  -24.0 -311.0  0.776370   \n",
      "1999975  0.489010   83.0 -247.0  -12.0  -17.0  -14.0  -24.0  0.846520   \n",
      "1999976  0.006849   23.0   83.0 -247.0  -12.0  -17.0  -14.0  0.893080   \n",
      "1999977 -0.046512    4.0   23.0   83.0 -247.0  -12.0  -17.0  0.938300   \n",
      "1999978  0.500000    2.0    4.0   23.0   83.0 -247.0  -12.0  0.939330   \n",
      "1999979 -0.166670    0.0    2.0    4.0   23.0   83.0 -247.0  0.941270   \n",
      "1999980  0.166670   -3.0    0.0    2.0    4.0   23.0   83.0  0.939240   \n",
      "1999981 -0.181820  -17.0   -3.0    0.0    2.0    4.0   23.0  0.952560   \n",
      "1999982 -0.463240 -379.0  -17.0   -3.0    0.0    2.0    4.0  0.958390   \n",
      "1999983 -0.083333  -58.0 -379.0  -17.0   -3.0    0.0    2.0  0.935790   \n",
      "1999984  0.000000    2.0  -58.0 -379.0  -17.0   -3.0    0.0 -0.994140   \n",
      "1999985 -0.772730  -50.0    2.0  -58.0 -379.0  -17.0   -3.0  0.931320   \n",
      "1999986 -0.277780  -13.0  -50.0    2.0  -58.0 -379.0  -17.0  0.926850   \n",
      "1999987 -0.180000  -15.0  -13.0  -50.0    2.0  -58.0 -379.0  0.941840   \n",
      "1999988 -0.470300 -302.0  -15.0  -13.0  -50.0    2.0  -58.0  0.907960   \n",
      "1999989  0.238100    8.0 -302.0  -15.0  -13.0  -50.0    2.0  0.940000   \n",
      "1999990 -0.210530  273.0    8.0 -302.0  -15.0  -13.0  -50.0 -0.428270   \n",
      "1999991 -0.500000  -61.0  273.0    8.0 -302.0  -15.0  -13.0 -0.524550   \n",
      "1999992 -0.500000    2.0  -61.0  273.0    8.0 -302.0  -15.0 -0.507590   \n",
      "1999993 -0.500000   62.0    2.0  -61.0  273.0    8.0 -302.0 -0.410100   \n",
      "1999994 -0.500000  -99.0   62.0    2.0  -61.0  273.0    8.0 -0.565510   \n",
      "1999995 -0.500000    5.0  -99.0   62.0    2.0  -61.0  273.0 -0.544660   \n",
      "1999996 -0.500000    0.0    5.0  -99.0   62.0    2.0  -61.0 -0.543480   \n",
      "1999997 -0.500000  -11.0    0.0    5.0  -99.0   62.0    2.0 -0.560920   \n",
      "1999998 -0.285710    7.0  -11.0    0.0    5.0  -99.0   62.0 -0.551420   \n",
      "1999999  0.500000   11.0    7.0  -11.0    0.0    5.0  -99.0 -0.534130   \n",
      "2000000 -0.441180  -25.0   11.0    7.0  -11.0    0.0    5.0 -0.579420   \n",
      "\n",
      "               8         9         10        11        12    13  \n",
      "0        0.467320 -0.478410  0.993240  0.481480 -0.322540 -0.50  \n",
      "1        0.458700  0.467320 -0.478410  0.993240  0.481480 -1.75  \n",
      "2       -0.490380  0.458700  0.467320 -0.478410  0.993240 -1.00  \n",
      "3        0.435190 -0.490380  0.458700  0.467320 -0.478410 -0.70  \n",
      "4        0.849830  0.435190 -0.490380  0.458700  0.467320 -2.00  \n",
      "5        0.363640  0.849830  0.435190 -0.490380  0.458700 -1.20  \n",
      "6        0.958160  0.363640  0.849830  0.435190 -0.490380 -1.40  \n",
      "7       -0.266360  0.958160  0.363640  0.849830  0.435190 -1.60  \n",
      "8       -0.905350 -0.266360  0.958160  0.363640  0.849830 -0.70  \n",
      "9        0.763820 -0.905350 -0.266360  0.958160  0.363640 -0.90  \n",
      "10       0.825610  0.763820 -0.905350 -0.266360  0.958160 -1.10  \n",
      "11       0.644110  0.825610  0.763820 -0.905350 -0.266360 -1.30  \n",
      "12      -0.336490  0.644110  0.825610  0.763820 -0.905350 -0.40  \n",
      "13       0.562280 -0.336490  0.644110  0.825610  0.763820 -0.50  \n",
      "14       0.082993  0.562280 -0.336490  0.644110  0.825610 -0.60  \n",
      "15       0.094996  0.082993  0.562280 -0.336490  0.644110 -0.80  \n",
      "16       0.066971  0.094996  0.082993  0.562280 -0.336490 -1.00  \n",
      "17       0.002383  0.066971  0.094996  0.082993  0.562280 -1.10  \n",
      "18      -0.336110  0.002383  0.066971  0.094996  0.082993 -1.30  \n",
      "19      -0.963080 -0.336110  0.002383  0.066971  0.094996 -0.30  \n",
      "20       0.582420 -0.963080 -0.336110  0.002383  0.066971 -0.30  \n",
      "21       0.634570  0.582420 -0.963080 -0.336110  0.002383 -0.25  \n",
      "22       0.258430  0.634570  0.582420 -0.963080 -0.336110 -0.15  \n",
      "23      -0.145540  0.258430  0.634570  0.582420 -0.963080 -0.25  \n",
      "24      -0.289860 -0.145540  0.258430  0.634570  0.582420 -0.35  \n",
      "25      -0.430890 -0.289860 -0.145540  0.258430  0.634570  0.65  \n",
      "26       0.889470 -0.430890 -0.289860 -0.145540  0.258430  0.75  \n",
      "27       0.985620  0.889470 -0.430890 -0.289860 -0.145540 -0.25  \n",
      "28      -0.901640  0.985620  0.889470 -0.430890 -0.289860  0.75  \n",
      "29       0.988950 -0.901640  0.985620  0.889470 -0.430890 -0.35  \n",
      "...           ...       ...       ...       ...       ...   ...  \n",
      "1999971 -0.742520 -0.691530 -0.210340 -0.210340 -0.200000 -0.80  \n",
      "1999972 -0.770750 -0.742520 -0.691530 -0.210340 -0.210340 -0.90  \n",
      "1999973 -0.811320 -0.770750 -0.742520 -0.691530 -0.210340 -1.00  \n",
      "1999974 -0.845730 -0.811320 -0.770750 -0.742520 -0.691530  0.10  \n",
      "1999975  0.776370 -0.845730 -0.811320 -0.770750 -0.742520  0.10  \n",
      "1999976  0.846520  0.776370 -0.845730 -0.811320 -0.770750  0.10  \n",
      "1999977  0.893080  0.846520  0.776370 -0.845730 -0.811320  0.10  \n",
      "1999978  0.938300  0.893080  0.846520  0.776370 -0.845730  0.10  \n",
      "1999979  0.939330  0.938300  0.893080  0.846520  0.776370  0.10  \n",
      "1999980  0.941270  0.939330  0.938300  0.893080  0.846520  0.20  \n",
      "1999981  0.939240  0.941270  0.939330  0.938300  0.893080  0.30  \n",
      "1999982  0.952560  0.939240  0.941270  0.939330  0.938300  0.40  \n",
      "1999983  0.958390  0.952560  0.939240  0.941270  0.939330  0.50  \n",
      "1999984  0.935790  0.958390  0.952560  0.939240  0.941270 -0.50  \n",
      "1999985 -0.994140  0.935790  0.958390  0.952560  0.939240  0.60  \n",
      "1999986  0.931320 -0.994140  0.935790  0.958390  0.952560  0.70  \n",
      "1999987  0.926850  0.931320 -0.994140  0.935790  0.958390  0.80  \n",
      "1999988  0.941840  0.926850  0.931320 -0.994140  0.935790  0.90  \n",
      "1999989  0.907960  0.941840  0.926850  0.931320 -0.994140  1.00  \n",
      "1999990  0.940000  0.907960  0.941840  0.926850  0.931320  0.00  \n",
      "1999991 -0.428270  0.940000  0.907960  0.941840  0.926850  0.00  \n",
      "1999992 -0.524550 -0.428270  0.940000  0.907960  0.941840  0.00  \n",
      "1999993 -0.507590 -0.524550 -0.428270  0.940000  0.907960  0.00  \n",
      "1999994 -0.410100 -0.507590 -0.524550 -0.428270  0.940000  0.00  \n",
      "1999995 -0.565510 -0.410100 -0.507590 -0.524550 -0.428270  0.00  \n",
      "1999996 -0.544660 -0.565510 -0.410100 -0.507590 -0.524550  0.00  \n",
      "1999997 -0.543480 -0.544660 -0.565510 -0.410100 -0.507590  0.00  \n",
      "1999998 -0.560920 -0.543480 -0.544660 -0.565510 -0.410100  0.00  \n",
      "1999999 -0.551420 -0.560920 -0.543480 -0.544660 -0.565510  0.00  \n",
      "2000000 -0.534130 -0.551420 -0.560920 -0.543480 -0.544660  0.00  \n",
      "\n",
      "[10000004 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=random.randint(0,100))\n",
    "df.to_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0      1      2      3      4      5      6         7   \\\n",
      "1318798  0.166670  152.0   -1.0  -10.0   59.0   58.0   11.0 -0.494180   \n",
      "1489300  0.481130   48.0   -7.0  -20.0   -4.0   73.0  -18.0 -0.583050   \n",
      "1085860 -0.237700  -16.0  131.0  149.0  442.0   -6.0  137.0 -0.865350   \n",
      "1666954 -0.587630  -93.0    9.0  648.0  108.0 -158.0  177.0  0.734660   \n",
      "617569   0.119050  -35.0   10.0  -28.0  204.0   80.0   75.0 -0.643560   \n",
      "1490234 -0.088235  -43.0  -36.0 -213.0  -18.0    1.0   33.0 -0.269010   \n",
      "1026533 -0.388890    5.0    2.0    3.0    6.0    0.0   -7.0  0.233850   \n",
      "240479   0.448720    1.0    4.0   97.0  -33.0    1.0  -31.0  0.533330   \n",
      "432147  -0.345070 -157.0  -25.0 -334.0  -66.0  -97.0  -26.0 -0.622000   \n",
      "528424   0.071429   12.0   25.0  -57.0  -63.0    1.0   -3.0  0.421830   \n",
      "1987514 -0.300000  -17.0    2.0   59.0   52.0  -27.0  -39.0 -0.469770   \n",
      "1524516  0.010000 -261.0   -5.0  -34.0  -25.0   -6.0  158.0  0.024299   \n",
      "1485488 -0.456520   11.0  -13.0    5.0  -19.0 -183.0    6.0 -0.157680   \n",
      "1151472  0.500000  -22.0  -10.0   -3.0  -21.0   -8.0  -31.0 -0.044944   \n",
      "1791580  0.000000  102.0  169.0   46.0    9.0   24.0    4.0 -0.587880   \n",
      "834980  -0.500000   -4.0  -62.0 -100.0  -49.0    4.0   33.0 -0.301850   \n",
      "1996153 -0.388890   -5.0  181.0  133.0   97.0   19.0   65.0 -0.275140   \n",
      "1906775  0.107140   66.0  -17.0 -121.0   27.0   -9.0  -80.0 -0.289050   \n",
      "1233918 -0.433330  -26.0  -47.0   12.0    1.0    6.0   11.0 -0.566840   \n",
      "1621591  0.000000    3.0   56.0   17.0  -10.0   -1.0   10.0 -0.127070   \n",
      "145604   0.409090  -37.0  -13.0    2.0  -13.0   -2.0   -2.0  0.644600   \n",
      "1816326 -0.174420  -54.0  -80.0  -82.0 -137.0   31.0   68.0 -0.700000   \n",
      "1980408  0.500000   33.0    4.0   31.0   29.0   37.0   -1.0  0.685710   \n",
      "275519  -0.166670   -2.0   -1.0  -12.0   -2.0   25.0   -2.0 -0.981400   \n",
      "759008   0.856450  100.0  250.0 -105.0 -560.0   -5.0  -14.0 -0.688470   \n",
      "1860888 -0.050000  -25.0 -138.0  113.0   21.0  -31.0  -18.0  0.340000   \n",
      "1174070 -0.115380    6.0   10.0    1.0  -22.0    5.0   58.0 -0.863380   \n",
      "764653   0.352460   68.0  125.0 -134.0  -34.0   35.0  -17.0  0.250940   \n",
      "109973   0.409090    7.0   12.0  -58.0   34.0   29.0    0.0  0.208630   \n",
      "1761470  0.461540   22.0   88.0    3.0    7.0    2.0   -6.0  0.135560   \n",
      "...           ...    ...    ...    ...    ...    ...    ...       ...   \n",
      "348726   0.198410   18.0   27.0    0.0  -20.0 -104.0   -9.0 -0.067961   \n",
      "1583710 -0.342110  -28.0    8.0   -2.0   -3.0   -2.0   -3.0  0.908810   \n",
      "351339  -0.500000   -4.0    6.0    2.0   12.0    5.0   -3.0  0.483150   \n",
      "88471   -0.500000   11.0   12.0    4.0    2.0   22.0   -3.0 -0.875650   \n",
      "426240   0.500000    0.0   -3.0    0.0   10.0   33.0   -3.0  0.683820   \n",
      "423995   0.071429   -8.0   -3.0 -120.0    4.0   63.0   -5.0  0.158270   \n",
      "1901499  0.500000    6.0   -5.0    0.0  -20.0   -3.0   75.0  0.284280   \n",
      "697586  -0.500000  -12.0  -17.0    2.0   -2.0    1.0   -4.0  0.746270   \n",
      "566412  -0.338710  250.0  340.0  154.0  304.0  173.0   17.0 -0.573360   \n",
      "409249  -0.522120 -125.0 -131.0  304.0   -1.0    3.0  -96.0 -0.726480   \n",
      "511835   0.500000    9.0    4.0   -1.0  103.0   60.0   -1.0  0.214290   \n",
      "164514   0.250000  -50.0   58.0    3.0  -38.0    1.0   -4.0 -0.086957   \n",
      "874590  -0.500000    6.0  154.0  -15.0    1.0    6.0    3.0  0.581080   \n",
      "752766   0.319440   35.0  -32.0   -5.0  -61.0    8.0   45.0  0.662580   \n",
      "734991  -0.437500  -18.0   10.0  -11.0  236.0   40.0  -64.0 -0.349160   \n",
      "1536815 -0.500000   84.0    4.0   56.0   55.0  -81.0   86.0 -0.485490   \n",
      "1982273  0.472400  785.0  748.0  -49.0  194.0  284.0  320.0 -0.953420   \n",
      "894765   0.386360  103.0  -17.0  -79.0  -23.0   17.0   32.0 -0.569280   \n",
      "1079719  0.500000  -45.0  -71.0    5.0   -0.5    1.0   -5.0  0.532910   \n",
      "1613283 -0.125000   -4.0    5.0  -14.0  -41.0    7.0 -175.0 -0.045226   \n",
      "1048423 -0.203910  -56.0   10.0  -12.0   41.0   16.0   53.0 -0.333330   \n",
      "1090176 -0.333330   -1.0   28.0 -143.0   55.0    4.0  -82.0  0.232640   \n",
      "1472032 -0.375000   -6.0   26.0   -2.0   -3.0    2.0   -1.0 -0.860810   \n",
      "1841066  0.416220  459.0    4.0   19.0   88.0  449.0   70.0 -0.957330   \n",
      "1400315  0.300000   29.0   53.0    8.0   32.0   37.0  107.0 -0.443480   \n",
      "559407   0.187500   -6.0  -23.0   23.0  -40.0  -23.0    2.0  0.778850   \n",
      "1078895  0.500000  -20.0    7.0  -26.0   -3.0   -3.0   -2.0  0.462180   \n",
      "865994  -0.470930  -85.0  130.0   14.0  -26.0  553.0  -35.0  0.083744   \n",
      "1192878  0.500000   73.0   57.0   24.0    2.0   66.0    1.0  0.305430   \n",
      "316983   0.818180   13.0   -5.0    5.0   -7.0   11.0   -5.0 -0.974080   \n",
      "\n",
      "               8         9         10        11        12    13  \n",
      "1318798  0.934960  0.852940 -0.848050 -0.848920 -0.960140 -0.20  \n",
      "1489300 -0.632260 -0.628060 -0.615510 -0.638940 -0.861110  0.00  \n",
      "1085860 -0.812140 -0.850550  0.644440 -0.210710 -0.197740  0.00  \n",
      "1666954 -0.966600  0.815130  0.722520  0.481120  0.098266 -0.40  \n",
      "617569  -0.527640 -0.549870 -0.496300  0.602390  0.477520 -0.40  \n",
      "1490234 -0.153610 -0.036415  0.348430  0.356210  0.355160 -0.10  \n",
      "1026533  0.223210  0.217780  0.210640  0.198220  0.198220  0.40  \n",
      "240479   0.307190  0.284770  0.797550 -0.347760 -0.913040 -0.15  \n",
      "432147  -0.450790 -0.402800  0.230770  0.420900 -0.401090  0.20  \n",
      "528424   0.403680  0.368420  0.441490  0.477610  0.475350  0.00  \n",
      "1987514 -0.442110 -0.457450 -0.582330  0.782260 -0.248550  0.30  \n",
      "1524516 -0.542730 -0.522730 -0.415250 -0.360000 -0.343040  0.00  \n",
      "1485488 -0.185500 -0.155460 -0.168440 -0.122950  0.262260  0.00  \n",
      "1151472 -0.014493  0.000000  0.004431  0.036585  0.048632  0.00  \n",
      "1791580  0.681590 -0.296970 -0.612900 -0.684210  0.239580 -0.05  \n",
      "834980  -0.292430 -0.189700  0.036053  0.142260  0.132780 -0.10  \n",
      "1996153 -0.268130 -0.615050  0.934980  0.686060  0.620870  0.00  \n",
      "1906775 -0.494550 -0.460530 -0.152660 -0.210910 -0.197780  0.00  \n",
      "1233918 -0.500810 -0.451720 -0.465990 -0.468480 -0.466000  0.00  \n",
      "1621591 -0.133330 -0.264460 -0.290580 -0.265230 -0.265870  0.00  \n",
      "145604   0.725490  0.775580  0.784510  0.860140  0.789810  0.00  \n",
      "1816326 -0.627780 -0.593500 -0.235290  0.227410  0.250000 -0.70  \n",
      "1980408  0.364160  0.329610  0.134620 -0.004255 -0.136690  1.10  \n",
      "275519  -0.963130 -0.954130 -0.809920 -0.795080  0.818180 -1.00  \n",
      "759008   0.889470  0.741700 -0.930810 -0.641790 -0.635190  0.00  \n",
      "1860888  0.369010  0.448800  0.346980  0.326190  0.366730  0.70  \n",
      "1174070 -0.884840 -0.877090 -0.880600 -0.762710 -0.769880  0.00  \n",
      "764653  -0.003802 -0.887320 -0.977560 -0.891640  0.850220  1.00  \n",
      "109973   0.193920  0.141820  0.455400  0.259260  0.141670 -0.90  \n",
      "1761470  0.113790  0.035144  0.032774  0.026772  0.024961  0.00  \n",
      "...           ...       ...       ...       ...       ...   ...  \n",
      "348726  -0.123080 -0.240820 -0.361960 -0.230770 -0.821340  0.65  \n",
      "1583710  0.900830  0.873970  0.884300  0.885250  0.890710  0.20  \n",
      "351339   0.505380  0.431580  0.428570  0.262140  0.220000 -0.40  \n",
      "88471   -0.935660  0.753850  0.691180  0.534880  0.432100 -0.30  \n",
      "426240   0.683820  0.707870  0.707870  0.646210  0.464970  1.00  \n",
      "423995   0.179310  0.186440 -0.910110  0.968750  0.988790  0.70  \n",
      "1901499  0.277970  0.288890  0.286930  0.332160  0.354240  0.00  \n",
      "697586   0.791880  0.872680  0.867370  0.882040  0.881720  0.40  \n",
      "566412  -0.699010 -0.888650  0.582370 -0.088186 -0.319210  0.00  \n",
      "409249  -0.645960 -0.447770  0.063670  0.065163  0.061481 -0.50  \n",
      "511835   0.188510  0.183300  0.184780 -0.002227 -0.121270  0.00  \n",
      "164514  -0.928570 -0.949490 -0.319740 -0.118890 -0.122950  0.00  \n",
      "874590   0.580420  0.101690  0.200000  0.191180  0.163930  0.00  \n",
      "752766   0.507000  0.676190  0.641180 -0.915370 -0.897220  0.00  \n",
      "734991  -0.328610 -0.347700 -0.325810 -0.481940 -0.506490  0.55  \n",
      "1536815 -0.746100 -0.761800 -0.883670 -0.991190 -0.689720  0.00  \n",
      "1982273 -0.986180 -0.935900 -0.911190  0.872500  0.377050  0.00  \n",
      "894765  -0.656800 -0.638270 -0.459100 -0.443380 -0.458450  0.00  \n",
      "1079719  0.675800 -0.997970 -0.205920 -0.997980  0.975610  0.00  \n",
      "1613283 -0.034653 -0.045131 -0.011086  0.072289  0.058824  0.00  \n",
      "1048423  0.041534  0.009836  0.041783 -0.069149 -0.116670  0.00  \n",
      "1090176  0.232970  0.190080  0.396360  0.316280  0.308760  0.00  \n",
      "1472032 -0.814950  0.058824  0.333330  0.189190  0.128210 -0.80  \n",
      "1841066 -0.928540 -0.914840 -0.926910  0.446290 -0.150100  0.70  \n",
      "1400315 -0.451990 -0.550820 -0.572340 -0.631090 -0.706850  0.00  \n",
      "559407   0.793550  0.848430  0.860140 -0.909090 -0.809520  0.40  \n",
      "1078895  0.601850  0.561640  0.699530  0.716980  0.734600  0.30  \n",
      "865994   0.531250 -0.666670  0.422650  0.920140  0.873870 -0.10  \n",
      "1192878  0.169400  0.091405  0.056954  0.054305 -0.036390  1.00  \n",
      "316983   0.985840 -0.991620  0.980230 -0.981910  0.985710 -0.50  \n",
      "\n",
      "[10000004 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后续只需要从硬盘读入即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0      1      2      3      4      5      6        7        8  \\\n",
      "0  0.16667  152.0   -1.0  -10.0   59.0   58.0   11.0 -0.49418  0.93496   \n",
      "1  0.48113   48.0   -7.0  -20.0   -4.0   73.0  -18.0 -0.58305 -0.63226   \n",
      "2 -0.23770  -16.0  131.0  149.0  442.0   -6.0  137.0 -0.86535 -0.81214   \n",
      "3 -0.58763  -93.0    9.0  648.0  108.0 -158.0  177.0  0.73466 -0.96660   \n",
      "4  0.11905  -35.0   10.0  -28.0  204.0   80.0   75.0 -0.64356 -0.52764   \n",
      "\n",
      "         9       10       11        12   13  \n",
      "0  0.85294 -0.84805 -0.84892 -0.960140 -0.2  \n",
      "1 -0.62806 -0.61551 -0.63894 -0.861110  0.0  \n",
      "2 -0.85055  0.64444 -0.21071 -0.197740  0.0  \n",
      "3  0.81513  0.72252  0.48112  0.098266 -0.4  \n",
      "4 -0.54987 -0.49630  0.60239  0.477520 -0.4  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "df = df.drop(['Unnamed: 0'],axis=1)\n",
    "print(df[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单/多个进程生成单颗决策树\n",
    "对于每一棵决策树的生成，使用mse作为评判标准，当整棵树的mse指数不再下降，停止分支；\n",
    "\n",
    "max_depth参数控制树的深度；\n",
    "\n",
    "min_samples_split参数控制每个节点分裂最少需要的样本数，min_samples_leaf参数控制每个节点最小样本树，这两个参数可以防止发生过拟合；\n",
    "\n",
    "min_samples_split，min_samples_leaf越大越不容易过拟合，但min_samples_split>min_samples_leaf；\n",
    "\n",
    "parallel参数控制并行化，为True时并行，使用多进程寻找最佳分割属性/特征，每个进程针对一个特征寻找最佳分割点，再由主进程选出最佳分割特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count, freeze_support\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, feature_rate=1.0, max_depth=None, min_leaf_size=1, parallel=False):\n",
    "        self._x = None\n",
    "        self._y = None\n",
    "        self._col_names = None\n",
    "        self._feature_rate = feature_rate\n",
    "        self._max_depth = max_depth\n",
    "        self._min_leaf_size = min_leaf_size\n",
    "        self._parallel = parallel\n",
    "\n",
    "        self._split_feature = None\n",
    "        self._split_point = None\n",
    "        self._mse_score = None\n",
    "        self._val = None\n",
    "\n",
    "        self._left_node = None\n",
    "        self._right_node = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # 训练决策树，参数说明如下\n",
    "        # x，类型为dataFrame，列数为特征数\n",
    "        # y，类型为dataFrame或者list，列数为1\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        if type(y) != np.ndarray:\n",
    "            self._y = np.array(self._y)\n",
    "        self._col_names = self._x.columns\n",
    "        self._val = np.mean(self._y)\n",
    "        self._mse_score = np.power(self._y-self._val, 2).sum()/len(self._y)\n",
    "\n",
    "        # 如果到达一定深度，停止分裂\n",
    "        if (self._max_depth is not None) and self._max_depth < 2:\n",
    "            return self\n",
    "\n",
    "        # 随机选取feature_rate*col_nums个属性（避免过拟合），选出其中最优的\n",
    "        feature_num = int(self._feature_rate*len(self._col_names))\n",
    "        features = np.random.choice(len(self._col_names), feature_num)\n",
    "        if self._parallel:\n",
    "            self.find_split_parallel(features)\n",
    "        else:\n",
    "            for feature in features:\n",
    "                tmp_split_feature, tmp_split_point, tmp_mse_score = self.find_best_split_point(feature)\n",
    "                if tmp_mse_score < self._mse_score:\n",
    "                    self._split_feature = tmp_split_feature\n",
    "                    self._split_point = tmp_split_point\n",
    "                    self._mse_score = tmp_mse_score\n",
    "\n",
    "        # 如果是寻找最优属性过程中，没能够完成分裂（原因很多种，可能是min_leaf_size限制、mse无法减小限制等），说明该节点是叶节点\n",
    "        if (self._split_feature is None) or (self._split_point is None):\n",
    "            return self\n",
    "\n",
    "        # 选出最优的分割属性、分割点后分裂节点，递归生成子节点\n",
    "        left_nodes = np.nonzero(np.array(self._x.iloc[:, self._split_feature]) < self._split_point)[0]\n",
    "        right_nodes = np.nonzero(np.array(self._x.iloc[:, self._split_feature]) >= self._split_point)[0]\n",
    "        max_depth = self._max_depth - 1 if self._max_depth is not None else None\n",
    "        self._left_node = DecisionTree(feature_rate=self._feature_rate, max_depth=max_depth,\n",
    "                                       min_leaf_size=self._min_leaf_size)\n",
    "        self._left_node.fit(self._x.iloc[left_nodes], self._y[left_nodes])\n",
    "        self._right_node = DecisionTree(feature_rate=self._feature_rate, max_depth=max_depth,\n",
    "                                        min_leaf_size=self._min_leaf_size)\n",
    "        self._right_node.fit(self._x.iloc[right_nodes], self._y[right_nodes])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def find_split_parallel(self, features):\n",
    "        # 并行化寻找最佳分割点\n",
    "        workers = cpu_count()\n",
    "        try:\n",
    "            workers = min(workers, len(features))\n",
    "        except Exception:\n",
    "            print('please input correct n_job')\n",
    "        pool = Pool(processes=workers)\n",
    "        result = []\n",
    "        for feature in features:\n",
    "            result.append(pool.apply_async(self.find_best_split_point, (feature, )))\n",
    "        # 关闭进程池，等待子进程退出后，拿出结果\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        # 由于开启不同进程训练树时会发生tree变量的拷贝（而不是引用），所以最后还需要将结果值赋给trees\n",
    "        for res in result:\n",
    "            item = res.get()\n",
    "            if item[2] < self._mse_score:\n",
    "                self._split_feature = item[0]\n",
    "                self._split_point = item[1]\n",
    "                self._mse_score = item[2]\n",
    "\n",
    "    def find_best_split_point(self, feature):\n",
    "        # 根据选定特征feature寻找最佳分割点\n",
    "\n",
    "        points = list(np.argsort(self._x.iloc[:, feature]))\n",
    "        start = self._x.iloc[points[self._min_leaf_size-1], feature]\n",
    "\n",
    "        y_square_sum = np.power(self._y, 2).sum()\n",
    "        y_sum = self._y.sum()\n",
    "        y_n = len(self._y)\n",
    "        left_square_sum = np.power(self._y[0:self._min_leaf_size], 2).sum()\n",
    "        left_sum = self._y[0:self._min_leaf_size].sum()\n",
    "        left_n = self._min_leaf_size\n",
    "        right_square_sum = y_square_sum - left_square_sum\n",
    "        right_sum = y_sum - left_sum\n",
    "        right_n = y_n - left_n\n",
    "\n",
    "        tmp_split_feature = None\n",
    "        tmp_split_point = None\n",
    "        tmp_mse_score = self._mse_score\n",
    "        for i in range(self._min_leaf_size, len(self._y)-self._min_leaf_size+1):\n",
    "            xi, yi = self._x.iloc[points[i], feature], self._y[points[i]]\n",
    "            tmp = 0\n",
    "            if xi == start:\n",
    "                tmp = 1\n",
    "            if tmp == 0:\n",
    "                left_score = self.calculate_mse(left_square_sum, left_sum, left_n)\n",
    "                right_score = self.calculate_mse(right_square_sum, right_sum, right_n)\n",
    "                score_after_split = left_score*(left_n/y_n) + right_score*(right_n/y_n)\n",
    "                # 必须要比当前的mse score小才能分裂，不然就有可能出现y都一样但是仍然分裂的状况，因为此时score是相等的\n",
    "                if score_after_split < tmp_mse_score:\n",
    "                    tmp_split_feature = feature\n",
    "                    tmp_split_point = xi\n",
    "                    tmp_mse_score = score_after_split\n",
    "            left_square_sum = left_square_sum + yi ** 2\n",
    "            left_sum = left_sum + yi\n",
    "            left_n = left_n + 1\n",
    "            right_square_sum = right_square_sum - yi ** 2\n",
    "            right_sum = right_sum - yi\n",
    "            right_n = right_n - 1\n",
    "            start = xi\n",
    "        return tmp_split_feature, tmp_split_point, tmp_mse_score\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 预测决策树的参数如下：\n",
    "        # x，类型为array或者dataFrame，其中的每一项表示一个预测对象的特征\n",
    "        if type(x) != np.ndarray:\n",
    "            x = np.array(x)\n",
    "        return [self.predict_one(one) for one in x]\n",
    "\n",
    "    def predict_one(self, one):\n",
    "        if (self._split_feature is None) or (self._split_point is None):\n",
    "            return self._val\n",
    "        if one[self._split_feature] < self._split_point:\n",
    "            return self._left_node.predict_one(one)\n",
    "        return self._right_node.predict_one(one)\n",
    "\n",
    "    def __repr__(self):\n",
    "        attr = 'sample size: {} \\t value: {} \\t mse score: {}\\n'.format(len(self._y), self._val, self._mse_score)\n",
    "        if (self._split_feature is not None) and (self._split_point is not None):\n",
    "            attr = attr + 'split feature: {} \\t split point: {}\\n'.format(self._split_feature, self._split_point)\n",
    "        return attr\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_mse(y_square_sum, y_sum, y_n):\n",
    "        return (y_square_sum/y_n) - (y_sum**2)/(y_n**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单棵树测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(feature_rate=0.4, min_leaf_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample size: 100000 \t value: -0.0027179999999999995 \t mse score: 0.3426780942286538\n",
       "split feature: 7 \t split point: 0.020381"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(df.iloc[0:100000, 0:13], np.array(df.iloc[0:100000, 13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0250000000000001, 0.62]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.predict([[0,2,3,4,5,1,2,3,4,5,1,2,3], [21,22,3,-114,-5,1,2,1,4,5,1,2,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用多进程实现并行生成多棵决策树\n",
    "使用参数n_job调整并行程度，表示使用的并行进程个数，n_job是多少，就是用多少个并行进程生成决策树\n",
    "\n",
    "使用参数n_tree调整森林规模，n_tree表示最终森林中决策树的数量，n_tree/n_job表示每个进程应该生成的决策树数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：由于Windows下multiprocessing库直接在jupyter下调用会出错，所以实际上自己是将下面这段代码放到同目录的RF.py中，然后导入调用；\n",
    "但是为了展示，这里也给出代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    def __init__(self, feature_rate=1.0, max_depth=None, min_leaf_size=1, n_job=None, n_tree=1, sample_size=None):\n",
    "        self._feature_rate = feature_rate\n",
    "        self._max_depth = max_depth\n",
    "        self._min_leaf_size = min_leaf_size\n",
    "        self._n_job = n_job\n",
    "        self._n_tree = n_tree\n",
    "        self._sample_size = sample_size\n",
    "\n",
    "        self._trees = [DecisionTree(self._feature_rate, self._max_depth, self._min_leaf_size) for i in\n",
    "                       range(self._n_tree)]\n",
    "        self._x = None\n",
    "        self._y = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # 训练森林，参数说明如下\n",
    "        # x，类型为dataFrame，列数为特征数\n",
    "        # y，类型为dataFrame，列数为1\n",
    "        self._x = x\n",
    "        self._y = y\n",
    "        if self._n_job:\n",
    "            self.fit_tree_parallel()\n",
    "        else:\n",
    "            for tree in self._trees:\n",
    "                self.fit_one_tree(tree, self._trees.index(tree))\n",
    "\n",
    "    def fit_tree_parallel(self):\n",
    "        # 并行训练森林中的树\n",
    "        workers = cpu_count()\n",
    "        try:\n",
    "            workers = min(workers, self._n_job)\n",
    "        except Exception:\n",
    "            print('please input correct n_job')\n",
    "        pool = Pool(processes=workers)\n",
    "        result = []\n",
    "        for tree in self._trees:\n",
    "            result.append(pool.apply_async(self.fit_one_tree, (tree, self._trees.index(tree))))\n",
    "        # 关闭进程池，等待子进程退出后，进行赋值\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        # 由于开启不同进程训练树时会发生tree变量的拷贝（而不是引用），所以最后还需要将结果值赋给trees\n",
    "        self._trees = [res.get() for res in result]\n",
    "\n",
    "    def fit_one_tree(self, tree, index):\n",
    "        # 训练单颗树的参数如下：\n",
    "        # tree，单棵树的引用，但被其他进程调用时是拷贝\n",
    "        # index，树的序号\n",
    "        start = datetime.datetime.now()\n",
    "        print('fit the {} th tree'.format(index))\n",
    "        # 首先进行有放回的随机筛选，选出sample_size个样本\n",
    "        sample_indexes = np.sort(np.random.choice(len(self._x), self._sample_size))\n",
    "        # 然后调用DecisionTree.fit进行训练\n",
    "        tree.fit(self._x.iloc[sample_indexes], self._y.iloc[sample_indexes])\n",
    "        print('{} th tree fit over, time used: {}'.format(index, (datetime.datetime.now()-start).seconds))\n",
    "        return tree\n",
    "\n",
    "    def predict(self, x):\n",
    "        # 预测决策树的参数如下：\n",
    "        # x，类型为dataFrame，其中的每一项表示一个预测对象的特征\n",
    "        all_predictions = [tree.predict(x) for tree in self._trees]\n",
    "        return np.mean(all_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This two line are jupyter magical method which use to auto reload the package you changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RF\n",
    "from multiprocessing import Pool, cpu_count, freeze_support\n",
    "# 这里其实是有输出提示（当前正在生成第几棵树，用时多少，但是由于代码在RF.py中，因此只能在jupyter的命令行那里看到）\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    rf = RF.RandomForest(feature_rate=0.3, max_depth=32, min_leaf_size=2, n_job=4, n_tree=10, sample_size=50000)\n",
    "    rf.fit(df.iloc[0:100000, 0:13], df.iloc[0:100000, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29666667, 0.29666667])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict([[1,2,3,4,5,1,2,3,4,5,1,2,3], [1,2,3,14,5,1,2,1,4,5,1,2,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用r2 score计算得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    y_mean = np.array(labels).mean()\n",
    "    SSR = (np.power(np.array(preds) - np.array(labels), 2)).sum()\n",
    "    SST = (np.power(np.array(labels) - y_mean, 2)).sum()\n",
    "    print('score', 1-SSR/SST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算分测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 0.2095079674884449\n"
     ]
    }
   ],
   "source": [
    "r2_score(rf.predict(df.iloc[0:100000, 0:13]), np.array(df.iloc[0:100000, 13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即在训练集上得分为0.21，接下来在测试集进行预测，并将结果写入文件，然后提交到竞赛链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用训练数据训练随机森林，然后做出预测并将结果写入磁盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RF\n",
    "from multiprocessing import Pool, cpu_count, freeze_support\n",
    "# 这里其实是有输出提示（当前正在生成第几棵树，用时多少，但是由于代码在RF.py中，因此只能在jupyter的命令行那里看到）\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()\n",
    "    rf = RF.RandomForest(feature_rate=0.3, max_depth=32, min_leaf_size=50, n_job=8, n_tree=200, sample_size=2000000)\n",
    "    rf.fit(df.iloc[:, 0:13], df.iloc[:, 13])\n",
    "\n",
    "    preds = []\n",
    "    for i in range(1, 7):\n",
    "        test_data = pd.read_csv('./data/test'+str(i)+'.csv', header=None)\n",
    "        tmp = rf.predict(test_data)\n",
    "        preds.extend(tmp)\n",
    "\n",
    "    ids = np.arange(1, len(preds)+1)\n",
    "    res = pd.DataFrame({'id':ids, 'Predicted':preds})\n",
    "    res.to_csv('./sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                id  Predicted\n",
      "0                1  -0.181492\n",
      "1                2  -0.123607\n",
      "2                3  -0.156439\n",
      "3                4  -0.210649\n",
      "4                5  -0.116018\n",
      "5                6  -0.042638\n",
      "6                7  -0.043169\n",
      "7                8  -0.157669\n",
      "8                9  -0.157335\n",
      "9               10  -0.156329\n",
      "10              11  -0.065739\n",
      "11              12  -0.159431\n",
      "12              13  -0.172382\n",
      "13              14  -0.161974\n",
      "14              15  -0.195837\n",
      "15              16  -0.269340\n",
      "16              17  -0.204745\n",
      "17              18  -0.165826\n",
      "18              19  -0.225185\n",
      "19              20  -0.226774\n",
      "20              21  -0.130646\n",
      "21              22  -0.252547\n",
      "22              23  -0.194690\n",
      "23              24  -0.161426\n",
      "24              25  -0.108898\n",
      "25              26  -0.092830\n",
      "26              27   0.182336\n",
      "27              28   0.277736\n",
      "28              29   0.249279\n",
      "29              30  -0.286592\n",
      "...            ...        ...\n",
      "10915091  10915092   0.145980\n",
      "10915092  10915093  -0.156992\n",
      "10915093  10915094  -0.234712\n",
      "10915094  10915095  -0.264812\n",
      "10915095  10915096  -0.099554\n",
      "10915096  10915097   0.078600\n",
      "10915097  10915098   0.143917\n",
      "10915098  10915099   0.414315\n",
      "10915099  10915100   0.339837\n",
      "10915100  10915101   0.132896\n",
      "10915101  10915102   0.289739\n",
      "10915102  10915103  -0.241237\n",
      "10915103  10915104  -0.373395\n",
      "10915104  10915105  -0.214932\n",
      "10915105  10915106  -0.124160\n",
      "10915106  10915107  -0.078829\n",
      "10915107  10915108   0.049119\n",
      "10915108  10915109   0.127610\n",
      "10915109  10915110  -0.066502\n",
      "10915110  10915111   0.344664\n",
      "10915111  10915112   0.247162\n",
      "10915112  10915113  -0.289007\n",
      "10915113  10915114  -0.235191\n",
      "10915114  10915115  -0.218436\n",
      "10915115  10915116  -0.247979\n",
      "10915116  10915117   0.114038\n",
      "10915117  10915118  -0.195214\n",
      "10915118  10915119  -0.082785\n",
      "10915119  10915120   0.032596\n",
      "10915120  10915121   0.210565\n",
      "\n",
      "[10915121 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('./sub.csv')\n",
    "print(submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
